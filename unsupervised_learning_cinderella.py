# -*- coding: utf-8 -*-
"""Unsupervised_learning_cinderella.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13GmzVmxVD6Q4ZqqjJ4hHvDTzUuEcT1hg
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
!pip install geopandas
!pip install folium

import numpy as np

file_path = '/content/drive/My Drive/Cinderella_II_Dataset.csv'
data = pd.read_csv(file_path)

data.shape

sample_frac = 0.4
sampled_data = data.sample(frac=sample_frac, random_state=42)

numeric_data = sampled_data.select_dtypes(include=['float64', 'int64'])

Q1 = numeric_data.quantile(0.25)
Q3 = numeric_data.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = ((numeric_data < lower_bound) | (numeric_data > upper_bound)).any(axis=1)
print("Number of Outliers in Numeric Data:", outliers.sum())

# Capping outliers using the 1st and 99th percentiles
for col in numeric_data.columns:
    lower_cap = sampled_data[col].quantile(0.01)
    upper_cap = sampled_data[col].quantile(0.99)
    sampled_data[col] = sampled_data[col].clip(lower_cap, upper_cap)

from sklearn.preprocessing import StandardScaler

# Selecting numerical columns
numerical_columns = sampled_data.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Applying StandardScaler
scaler = StandardScaler()
sampled_data[numerical_columns] = scaler.fit_transform(sampled_data[numerical_columns])

print(sampled_data[numerical_columns].head())

from sklearn.preprocessing import OneHotEncoder
# Categorical columns that need encoding
categorical_columns = ['sailing_state', 'closest_dock', 'segment_state', 'leg']

# Applying OneHotEncoder
encoder = OneHotEncoder(sparse=False)
encoded_data = encoder.fit_transform(sampled_data[categorical_columns])

encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))

sampled_data = sampled_data.reset_index(drop=True)
encoded_df = encoded_df.reset_index(drop=True)
sampled_data_encoded = pd.concat([sampled_data, encoded_df], axis=1)

# Dropping the original categorical columns as they are now encoded
sampled_data_encoded.drop(categorical_columns, axis=1, inplace=True)
print(sampled_data_encoded.head())



# Convert Timestamp to datetime
sampled_data_encoded['Timestamp'] = pd.to_datetime(data['Timestamp'])

# Extracting temporal features
sampled_data_encoded['hour'] = sampled_data_encoded['Timestamp'].dt.hour
sampled_data_encoded['day_of_week'] = sampled_data_encoded['Timestamp'].dt.dayofweek
sampled_data_encoded['month'] = sampled_data_encoded['Timestamp'].dt.month

# Function to calculate distance based on latitude and longitude
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Radius of the Earth in km
    dLat = np.radians(lat2 - lat1)
    dLon = np.radians(lon2 - lon1)
    a = np.sin(dLat/2) * np.sin(dLat/2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dLon/2) * np.sin(dLon/2)
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
    distance = R * c
    return distance

# Calculate distance traveled between consecutive readings
sampled_data_encoded['prev_latitude'] = sampled_data_encoded['latitude'].shift(1)
sampled_data_encoded['prev_longitude'] = sampled_data_encoded['longitude'].shift(1)
sampled_data_encoded['distance_traveled'] = sampled_data_encoded.apply(lambda row: haversine(row['prev_latitude'], row['prev_longitude'],
                                                             row['latitude'], row['longitude']), axis=1)

# Drop temporary columns and rows with NaN values created by shift operation
sampled_data_encoded.drop(['prev_latitude', 'prev_longitude'], axis=1, inplace=True)
sampled_data_encoded.dropna(subset=['distance_traveled'], inplace=True)

# List of key features including those identified from correlation analysis
key_features = [
    'engine_rpm', 'latitude', 'longitude',
    'speedOverGround', 'speedApparent', 'windSpeed_Onb',
    'distanceFromDock', 'delta_distance', 'hour',
    'day_of_week', 'month', 'distance_traveled'
]

# Creating interaction terms (example: engine_rpm * speedOverGround)
sampled_data_encoded['rpm_speed_interaction'] = sampled_data_encoded['engine_rpm'] * sampled_data_encoded['speedOverGround']

# Creating the new DataFrame with key features and interaction terms
key_features_dataset = sampled_data_encoded[key_features + ['rpm_speed_interaction']]

print(key_features_dataset.head())

from sklearn.preprocessing import StandardScaler

# Standardizing the features
scaler = StandardScaler()
key_features_scaled = scaler.fit_transform(key_features_dataset)

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)
key_features_reduced = pca.fit_transform(key_features_scaled)

from sklearn.cluster import KMeans

# Apply K-Means Clusterings
kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(key_features_reduced)

# Add the cluster information back to the DataFrame
key_features_dataset['cluster'] = clusters

# Analyze cluster characteristics
cluster_characteristics = key_features_dataset.groupby('cluster').mean()

print(cluster_characteristics)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.scatterplot(x=key_features_reduced[:, 0], y=key_features_reduced[:, 1], hue=clusters, palette="viridis")
plt.title('Vessel Operational Modes Clustering')
plt.show()

from sklearn.metrics import silhouette_score

# Calculate silhouette score
silhouette_avg = silhouette_score(key_features_reduced, clusters)
print(f"The average silhouette score for the current clustering is: {silhouette_avg}")

"""DBSCAN

"""

from sklearn.neighbors import NearestNeighbors

# Determine the distance to the k-th nearest neighbor
nearest_neighbors = NearestNeighbors(n_neighbors=5)
nearest_neighbors.fit(key_features_reduced)
distances, indices = nearest_neighbors.kneighbors(key_features_reduced)

sorted_distances = np.sort(distances[:, 4], axis=0)  # 4 corresponds to the 5th nearest neighbor
plt.plot(sorted_distances)
plt.xlabel("Points")
plt.ylabel("Distance to 5th Nearest Neighbor")
plt.title("k-Distance Graph")
plt.show()

"""The graph remains relatively flat, indicating a small distance to the 5th nearest neighbor for most points, and then sharply increases, which suggests a transition to a lower density region in the data. The elbow, or optimal eps value, is not distinctly marked but appears to be in the lower third of the plot where the slope starts to increase more noticeably. This suggests that the majority of the data points have a relatively tight grouping at a lower eps value, before reaching the steep ascent which likely corresponds to outliers or less dense regions. The precise eps value where this change occurs would be the starting point for fine-tuning DBSCAN's eps parameter, aiming to maximize cluster separation and minimize noise identification.

"""



from sklearn.cluster import DBSCAN

estimated_eps_value = 0.5

# Apply DBSCAN with the estimated eps value
dbscan = DBSCAN(eps=estimated_eps_value, min_samples=5)
clusters = dbscan.fit_predict(key_features_reduced)

# Evaluate the results
n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
n_noise = list(clusters).count(-1)
if n_clusters > 1:
    silhouette_avg = silhouette_score(key_features_reduced[clusters != -1], clusters[clusters != -1])
else:
    silhouette_avg = None

print(f"Estimated eps: {estimated_eps_value}")
print(f"Number of clusters formed: {n_clusters}")
print(f"Number of noise points: {n_noise}")
print(f"Silhouette score: {silhouette_avg}")

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# Increasing the range for eps
eps_values = [0.95, 1.0, 1.05, 1.1, 1.15, 1.2]
min_samples_values = [11, 13, 15, 17, 19, 21]  # Incrementing min_samples

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        clusters = dbscan.fit_predict(key_features_reduced)

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
import numpy as np

# Assuming key_features_reduced is your dataset
# key_features_reduced = ...

# Defining the expanded range for eps and the reduced range for min_samples
eps_values = [1.2, 1.25, 1.3, 1.35, 1.4,1.45, 1.6, 1.7]
min_samples_values = [5, 7, 9, 10]

results = []

for eps in eps_values:
    for min_samples in min_samples_values:
        # Apply DBSCAN with the current set of parameters
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(key_features_reduced)

        # Count the number of clusters and noise points
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)

        # Compute the silhouette score if there are sufficient clusters
        if 1 < n_clusters < len(labels):
            silhouette = silhouette_score(key_features_reduced, labels)
        else:
            silhouette = None

        results.append({
            'eps': eps,
            'min_samples': min_samples,
            'clusters': n_clusters,
            'noise': n_noise,
            'silhouette': silhouette
        })

# Sorting results by silhouette score, ignoring None values
results = sorted





























