# -*- coding: utf-8 -*-
"""supervised_learning_cinderella.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14TUpmcm-2qEWFlMoaEV3wfCgtffwiVyW
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
!pip install geopandas
!pip install folium

import pandas as pd

file_path = '/content/drive/My Drive/Cinderella_II_Dataset.csv'
data = pd.read_csv(file_path)
data.head()

print("Dataset Shape:", data.shape)
print("\nColumn Names:", data.columns.tolist())
print("\nData Types:\n", data.dtypes)
print("\nMissing Values:\n", data.isnull().sum())
print("\nStatistical Summary:\n", data.describe())

# Correlation Matrix
correlation_matrix = data.corr()
plt.figure(figsize=(15, 15))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Convert Timestamp to datetime
data['Timestamp'] = pd.to_datetime(data['Timestamp'])

#time series plot for 'engine_fuel_rate'
plt.figure(figsize=(15, 6))
plt.plot(data['Timestamp'], data['engine_fuel_rate'])
plt.title('Engine Fuel Rate Over Time')
plt.xlabel('Timestamp')
plt.ylabel('Engine Fuel Rate')
plt.show()

# Histogram for 'engine_fuel_rate'
plt.hist(data['engine_fuel_rate'], bins=30)
plt.title('Distribution of Engine Fuel Rate')
plt.xlabel('Engine Fuel Rate')
plt.ylabel('Frequency')
plt.show()

import folium

# Create a base map
m = folium.Map(location=[59.420199, 18.429791], zoom_start=10)  # Use the mean latitude and longitude from your dataset

for idx, row in data.iterrows():
    folium.CircleMarker(location=[row['latitude'], row['longitude']],
                        radius=1,
                        fill=True).add_to(m)
m

# Correlation between weather conditions and fuel consumption
weather_columns = ['windSpeed_Onb', 'waveHeight', 'currentSpeed', 'Temp_2m', 'Press_surf', 'Precip']
fuel_consumption = 'engine_fuel_rate'

for col in weather_columns:
    data.plot(kind='scatter', x=col, y=fuel_consumption, alpha=0.5)
    plt.title(f'Correlation between {col} and Fuel Consumption')
    plt.xlabel(col)
    plt.ylabel(fuel_consumption)
    plt.show()

course_variables = ['courseOverGroundTrue', 'heading_magnetic', 'course_heading_diff']

for var in course_variables:
    data.plot(kind='scatter', x=var, y=fuel_consumption, alpha=0.5)
    plt.title(f'Correlation between {var} and Fuel Consumption')
    plt.xlabel(var)
    plt.ylabel(fuel_consumption)
    plt.show()



"""1.	Data Cleaning:"""

missing_values = data.isnull().sum()
print("Missing Values:\n", missing_values)

"""outlier detection"""

numeric_data = data.select_dtypes(include=['float64', 'int64'])

Q1 = numeric_data.quantile(0.25)
Q3 = numeric_data.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = ((numeric_data < lower_bound) | (numeric_data > upper_bound)).any(axis=1)
print("Number of Outliers in Numeric Data:", outliers.sum())

# Capping outliers using the 1st and 99th percentiles
for col in numeric_data.columns:
    lower_cap = data[col].quantile(0.01)
    upper_cap = data[col].quantile(0.99)
    data[col] = data[col].clip(lower_cap, upper_cap)

summary_after_capping = data.describe()
print(summary_after_capping)

import matplotlib.pyplot as plt
import seaborn as sns

key_columns = ['engine_fuel_rate', 'speedOverGround', 'windSpeed_Onb']

for col in key_columns:
    plt.figure(figsize=(10, 6))
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col} After Capping Outliers')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

"""

```
Feature Engineering Suggestions
```

"""

import pandas as pd
import numpy as np


data['Timestamp'] = pd.to_datetime(data['Timestamp'])

# Extracting temporal features
data['hour'] = data['Timestamp'].dt.hour
data['day_of_week'] = data['Timestamp'].dt.dayofweek
data['month'] = data['Timestamp'].dt.month

# Function to calculate distance based on latitude and longitude
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Radius of the Earth in km
    dLat = np.radians(lat2 - lat1)
    dLon = np.radians(lon2 - lon1)
    a = np.sin(dLat/2) * np.sin(dLat/2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dLon/2) * np.sin(dLon/2)
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
    distance = R * c
    return distance

# Calculate distance traveled between consecutive readings
data['prev_latitude'] = data['latitude'].shift(1)
data['prev_longitude'] = data['longitude'].shift(1)
data['distance_traveled'] = data.apply(lambda row: haversine(row['prev_latitude'], row['prev_longitude'],
                                                             row['latitude'], row['longitude']), axis=1)

# Drop temporary columns and rows with NaN values created by shift operation
data.drop(['prev_latitude', 'prev_longitude'], axis=1, inplace=True)
data.dropna(subset=['distance_traveled'], inplace=True)

# Display the range (min and max) and data type of each column
for column in data.columns:
    if data[column].dtype in ['float64', 'int64']:
        print(f"{column}: Min = {data[column].min()}, Max = {data[column].max()}, Data type = {data[column].dtype}")

# Check for categorical variables
for column in data.columns:
    if data[column].dtype == 'object':
        print(f"Categorical variable found: {column}, Unique values: {data[column].nunique()}")

from sklearn.preprocessing import StandardScaler

# Selecting numerical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Applying StandardScaler
scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

print(data[numerical_columns].head())

from sklearn.preprocessing import OneHotEncoder

# Categorical columns that need encoding
categorical_columns = ['sailing_state', 'closest_dock', 'segment_state', 'leg']

# Applying OneHotEncoder
encoder = OneHotEncoder(sparse=False)
encoded_data = encoder.fit_transform(data[categorical_columns])

encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))


data = data.reset_index(drop=True)
encoded_df = encoded_df.reset_index(drop=True)
data = pd.concat([data, encoded_df], axis=1)

# Dropping the original categorical columns as they are now encoded
data.drop(categorical_columns, axis=1, inplace=True)

print(data.head())



import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the correlation matrix
corr_matrix = data.corr()

plt.figure(figsize=(15, 15))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()







from sklearn.decomposition import PCA

numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns
data_numeric = data[numeric_columns]

# Applying PCA
pca = PCA(n_components=0.95)
data_reduced = pca.fit_transform(data_numeric)

print(f"Reduced data shape: {data_reduced.shape}")

data_reduced_df = pd.DataFrame(data_reduced, columns=[f'PC{i+1}' for i in range(data_reduced.shape[1])])
print(data_reduced_df.head())

import pandas as pd


# List of key features including those identified from correlation analysis
key_features = [
    'engine_rpm', 'latitude', 'longitude',
    'speedOverGround', 'speedApparent', 'windSpeed_Onb',
    'distanceFromDock', 'delta_distance', 'hour',
    'day_of_week', 'month', 'distance_traveled'
]

# Creating interaction terms
data['rpm_speed_interaction'] = data['engine_rpm'] * data['speedOverGround']

key_features_dataset = data[key_features + ['rpm_speed_interaction']]

print(key_features_dataset.head())

from sklearn.decomposition import PCA
import pandas as pd


# Standardizing the data before applying PCA
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(key_features_dataset)

# Applying PCA
pca = PCA(n_components=0.95)
data_reduced = pca.fit_transform(scaled_data)

principal_components = pd.DataFrame(
    data_reduced,
    columns=[f'PC{i+1}' for i in range(data_reduced.shape[1])]
)

print(principal_components.head())

print("\nVariance Ratio Explained by Each Component:")
print(pca.explained_variance_ratio_)

from sklearn.model_selection import train_test_split

y = data['engine_fuel_rate']  # Target variable
X = principal_components

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Creating the Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

# RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Squared Error: {rmse}")

# MAE
mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")

# R² Score
r2 = r2_score(y_test, y_pred)
print(f"R² Score: {r2}")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Comparison of Predictions and Ground Truth Labels')
plt.xlabel('Actual Fuel Consumption (Ground Truth)')
plt.ylabel('Predicted Fuel Consumption')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

"""new model

"""

# Weather-related features
weather_features = ['windSpeed_Onb', 'waveHeight', 'currentSpeed', 'Temp_2m', 'Press_surf']

# Speed-related features
speed_features = ['engine_rpm', 'speedOverGround', 'speedApparent']

# Separate the target variable and feature subsets
y = data['engine_fuel_rate']

# Data for weather model
X_weather = data[weather_features]

# Data for speed model
X_speed = data[speed_features]



from sklearn.model_selection import train_test_split

# Splitting the weather data
X_train_weather, X_test_weather, y_train_weather, y_test_weather = train_test_split(
    X_weather, y, test_size=0.2, random_state=42)

# Splitting the speed data
X_train_speed, X_test_speed, y_train_speed, y_test_speed = train_test_split(
    X_speed, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Model for weather-related features
model_weather = RandomForestRegressor(random_state=42)
model_weather.fit(X_train_weather, y_train_weather)
y_pred_weather = model_weather.predict(X_test_weather)
mse_weather = mean_squared_error(y_test_weather, y_pred_weather)
print(f"Weather Features Model - MSE: {mse_weather}")

# Model for speed-related features
model_speed = RandomForestRegressor(random_state=42)
model_speed.fit(X_train_speed, y_train_speed)
y_pred_speed = model_speed.predict(X_test_speed)
mse_speed = mean_squared_error(y_test_speed, y_pred_speed)
print(f"Speed Features Model - MSE: {mse_speed}")